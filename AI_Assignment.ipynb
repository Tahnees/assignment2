{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOm5FWerA9G43shjtCnSEpj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tahnees/assignment2/blob/main/AI_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------------COMPARISION----------------------------\n",
        "\n",
        "**1.**\n",
        "Implemented Code:\n",
        "- Focused on **image captioning** using a VisionEncoderDecoder model (`nlpconnect/vit-gpt2-image-captioning`).\n",
        "- Does not include **segmentation or multi-target reasoning** tasks.\n",
        "**Paper:**\n",
        "- Designed for **pixel-level reasoning** tasks, including segmentation and mask generation.\n",
        "- Utilizes a **novel lightweight pixel decoder** and segmentation codebook for high-quality mask production.\n",
        "\n",
        "**2.**\n",
        "PixelLM employs a more advanced architecture tailored for **segmentation** and **pixel-level reasoning**, while implemented model is optimized for **sequence-to-sequence caption generation**.\n",
        "\n",
        "**3.**\n",
        "PixelLM demonstrates **state-of-the-art segmentation performance**, while implemented code provides metrics for text-based evaluations (BLEU scores).\n",
        "\n",
        "**4.**\n",
        "**Implemented Code:**\n",
        "- Implements a standard image-captioning pipeline with enhancements like caption preprocessing and data augmentation.\n",
        "**Paper:**\n",
        "- Introduces innovative features such as:\n",
        "  - **Token fusion** for multi-target reasoning.\n",
        "  - **Target refinement loss** for overlapping masks.\n",
        "  - MUSE dataset tailored for segmentation tasks.\n",
        "\n",
        "**5.**:\n",
        "PixelLM introduces innovations to handle segmentation tasks effectively, whereas implemented code leverages existing methods for caption generation.\n",
        "\n",
        "**6.**:\n",
        "The paper uses COCO-Stuff as part of the training data but does not report accuracy specifically for this dataset. Instead, it focuses on benchmarks better aligned with the model's pixel-level reasoning objectives, like MUSE and refCOCO.\n",
        "Best Accuracy on MUSE Test Set (13B Model):\n",
        "gIoU: 45.2\n",
        "cIoU: 62.9.\n",
        "Best Accuracy on refCOCOg Validation Set (13B Model):\n",
        "gIoU: 70.5\n",
        "cIoU: 73.0\n",
        "\n",
        "Implemented code accuracy= 80.06"
      ],
      "metadata": {
        "id": "WeJEY3h4nlrW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpj9I-Hk4gA8"
      },
      "outputs": [],
      "source": [
        "from logging import lastResort\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from zipfile import ZipFile\n",
        "from PIL import Image\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import VisionEncoderDecoderModel, AutoTokenizer, AutoModelForVision2Seq\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.optim import AdamW\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "# =======================================\n",
        "#  Mount Google Drive\n",
        "# =======================================\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# =======================================\n",
        "#  Set Paths and Extract Dataset\n",
        "# =======================================\n",
        "drive_dataset_path = '/content/drive/My Drive/Images.zip'\n",
        "dataset_folder = './dataset'\n",
        "\n",
        "if not os.path.exists(dataset_folder):\n",
        "    with ZipFile(drive_dataset_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(dataset_folder)\n",
        "\n",
        "print(\"Extracted dataset contents:\", os.listdir(dataset_folder))\n",
        "\n",
        "nested_dirs = os.listdir(dataset_folder)\n",
        "if len(nested_dirs) == 1 and os.path.isdir(os.path.join(dataset_folder, nested_dirs[0])):\n",
        "    dataset_folder = os.path.join(dataset_folder, nested_dirs[0])\n",
        "\n",
        "print(\"Updated dataset folder path:\", dataset_folder)\n",
        "print(\"Contents of dataset folder:\", os.listdir(dataset_folder))\n",
        "\n",
        "# =======================================\n",
        "#  Verify and Load Captions and Images\n",
        "# =======================================\n",
        "captions_file = os.path.join(dataset_folder, 'captions.json')\n",
        "image_dir = os.path.join(dataset_folder, 'Images')\n",
        "\n",
        "if not os.path.exists(captions_file):\n",
        "    raise FileNotFoundError(f\"Captions file not found at {captions_file}. Check the dataset structure.\")\n",
        "if not os.path.exists(image_dir):\n",
        "    raise FileNotFoundError(f\"Image directory not found at {image_dir}. Check the dataset structure.\")\n",
        "\n",
        "with open(captions_file, 'r') as f:\n",
        "    captions_data = json.load(f)\n",
        "print(\"Number of captions loaded:\", len(captions_data))\n",
        "\n",
        "# =======================================\n",
        "#  EDA: Integrate Dataset Insights\n",
        "# =======================================\n",
        "caption_lengths = [len(caption[\"caption\"].split()) for caption in captions_data]\n",
        "print(f\"Caption Length Stats: Min: {min(caption_lengths)}, Max: {max(caption_lengths)}, Mean: {np.mean(caption_lengths):.2f}\")\n",
        "\n",
        "plt.hist(caption_lengths, bins=20, color='skyblue')\n",
        "plt.title(\"Caption Length Distribution\")\n",
        "plt.xlabel(\"Length\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n",
        "\n",
        "image_sizes = [Image.open(os.path.join(image_dir, f\"COCO_train2014_{str(c['image_id']).zfill(12)}.jpg\")).size for c in captions_data[:50]]\n",
        "widths, heights = zip(*image_sizes)\n",
        "print(\"Image Widths:\", pd.Series(widths).describe())\n",
        "print(\"Image Heights:\", pd.Series(heights).describe())\n",
        "\n",
        "plt.scatter(widths, heights, alpha=0.5)\n",
        "plt.title(\"Image Resolution Distribution\")\n",
        "plt.xlabel(\"Width\")\n",
        "plt.ylabel(\"Height\")\n",
        "plt.show()\n",
        "# =======================================\n",
        "#  Preprocess Captions\n",
        "# =======================================\n",
        "def preprocess_caption(caption):\n",
        "    caption = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", caption)\n",
        "    caption = caption.lower()\n",
        "    return caption\n",
        "\n",
        "for caption in captions_data:\n",
        "    caption['caption'] = preprocess_caption(caption['caption'])\n",
        "\n",
        "# =======================================\n",
        "#  Define Custom Dataset Class\n",
        "# =======================================\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "class CustomImageCaptionDataset(Dataset):\n",
        "    def __init__(self, image_dir, captions_data, transform=None, tokenizer=None, max_length=50):\n",
        "        self.image_dir = image_dir\n",
        "        self.captions_data = captions_data\n",
        "        self.transform = transform\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.captions_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        caption_data = self.captions_data[idx]\n",
        "        image_id = caption_data['image_id']\n",
        "        caption = caption_data['caption']\n",
        "\n",
        "        image_filename = f\"COCO_train2014_{str(image_id).zfill(12)}.jpg\"\n",
        "        image_path = os.path.join(self.image_dir, image_filename)\n",
        "\n",
        "        if not os.path.exists(image_path):\n",
        "            raise FileNotFoundError(f\"Image file not found: {image_path}\")\n",
        "\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        tokenized_caption = self.tokenizer.encode_plus(\n",
        "            caption,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return image, tokenized_caption[\"input_ids\"].squeeze(0), tokenized_caption[\"attention_mask\"].squeeze(0)\n",
        "test_caption = \"Sample caption for debugging tokenizer.\"\n",
        "encoded = tokenizer.encode(test_caption)\n",
        "decoded = tokenizer.decode(encoded)\n",
        "print(f\"Original: {test_caption}\")\n",
        "print(f\"Encoded: {encoded}\")\n",
        "print(f\"Decoded: {decoded}\")\n",
        "\n",
        "# =======================================\n",
        "#  Data Augmentation and Transforms\n",
        "# =======================================\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomCrop((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# =======================================\n",
        "#  Split Dataset into Train/Val/Test\n",
        "# =======================================\n",
        "indices = list(range(len(captions_data)))\n",
        "train_indices, val_test_indices = train_test_split(indices, test_size=0.3, random_state=42)\n",
        "val_indices, test_indices = train_test_split(val_test_indices, test_size=0.5, random_state=42)\n",
        "\n",
        "train_data = [captions_data[i] for i in train_indices]\n",
        "val_data = [captions_data[i] for i in val_indices]\n",
        "test_data = [captions_data[i] for i in test_indices]\n",
        "\n",
        "subset_size = 1000\n",
        "train_data_subset = train_data[:subset_size]\n",
        "train_dataset = CustomImageCaptionDataset(image_dir, train_data_subset, transform, tokenizer)\n",
        "val_dataset = CustomImageCaptionDataset(image_dir, val_data, transform, tokenizer)\n",
        "test_dataset = CustomImageCaptionDataset(image_dir, test_data, transform, tokenizer)\n",
        "\n",
        "# =======================================\n",
        "#  Define Custom Collate Function\n",
        "# =======================================\n",
        "def collate_fn(batch):\n",
        "    images, input_ids, attention_masks = zip(*batch)\n",
        "    images = torch.stack(images)\n",
        "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "    attention_masks = pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
        "    return images.to(device), input_ids.to(device), attention_masks.to(device)\n",
        "\n",
        "# =======================================\n",
        "#  Create DataLoaders\n",
        "# =======================================\n",
        "batch_size = 32\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "def visualize_images_with_captions(dataloader, num_batches=1):\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "\n",
        "    for i, (images, input_ids, attention_masks) in enumerate(dataloader):\n",
        "        print(f\"Batch {i + 1}:\")\n",
        "\n",
        "        for j in range(len(images)):\n",
        "            image = images[j]\n",
        "            caption = tokenizer.decode(input_ids[j], skip_special_tokens=True)\n",
        "\n",
        "            if isinstance(image, torch.Tensor):\n",
        "                image = image.permute(1, 2, 0).numpy()\n",
        "                image = (image * std) + mean\n",
        "                image = np.clip(image, 0, 1)\n",
        "            if not caption.strip():\n",
        "                print(\"Warning: Empty caption detected during visualization.\")\n",
        "\n",
        "            plt.imshow(image)\n",
        "            plt.axis('off')\n",
        "            plt.title(caption)\n",
        "            plt.show()\n",
        "            print(\"------------------------------\")\n",
        "        if i + 1 >= num_batches:\n",
        "            break\n",
        "\n",
        "dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "visualize_images_with_captions(dataloader, num_batches=2)\n",
        "\n",
        "# =======================================\n",
        "#  Debug: Iterate Through DataLoader\n",
        "# =======================================\n",
        "for i, (images, input_ids, attention_masks) in enumerate(train_loader):\n",
        "    print(f\"Batch {i + 1}:\")\n",
        "    print(f\"Images shape: {images.shape}\")\n",
        "    print(f\"Input IDs shape: {input_ids.shape}\")\n",
        "    print(f\"Attention Masks shape: {attention_masks.shape}\")\n",
        "    if i == 1:\n",
        "        break\n",
        "\n",
        "# =======================================\n",
        "#  Define the Model\n",
        "# =======================================\n",
        "model_checkpoint = \"google/pix2struct-base\"\n",
        "\n",
        "model =VisionEncoderDecoderModel.from_pretrained(model_checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "#model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "\n",
        "model.generation_config.max_length = 50\n",
        "model.generation_config.num_beams = 8\n",
        "model.config.eos_token_id = tokenizer.eos_token_id\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# =======================================\n",
        "#  Training and Validation Loops\n",
        "# =======================================\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "def compute_bleu_score(preds, labels, tokenizer):\n",
        "    bleu_scores = []\n",
        "    smooth_fn = SmoothingFunction().method1\n",
        "    for pred, label in zip(preds, labels):\n",
        "        pred_caption = tokenizer.decode(pred, skip_special_tokens=True)\n",
        "        true_caption = tokenizer.decode(label, skip_special_tokens=True)\n",
        "\n",
        "        pred_tokens = pred_caption.split()\n",
        "        true_tokens = true_caption.split()\n",
        "\n",
        "        bleu_scores.append(sentence_bleu([true_tokens], pred_tokens, smoothing_function=smooth_fn))\n",
        "\n",
        "    return np.mean(bleu_scores)\n",
        "\n",
        "def train_model_with_accuracy(model, train_loader, val_loader, criterion, optimizer, tokenizer, num_epochs=2):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        total_correct_tokens, total_tokens = 0, 0\n",
        "\n",
        "        for i, (images, input_ids, attention_masks) in enumerate(train_loader):\n",
        "            images = images.to(device)\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_masks = attention_masks.to(device)\n",
        "\n",
        "            outputs = model(pixel_values=images, labels=input_ids)\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            preds = torch.argmax(logits, dim=-1)\n",
        "            correct_tokens = (preds == input_ids).sum().item()\n",
        "            total_tokens += input_ids.numel()\n",
        "            total_correct_tokens += correct_tokens\n",
        "\n",
        "            if i % 10 == 0:\n",
        "                print(f\"Step {i}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        train_accuracy = total_correct_tokens / total_tokens * 100\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%\")\n",
        "\n",
        "        val_loss, val_preds, val_labels = evaluate_model(model, val_loader, tokenizer, criterion, device)\n",
        "\n",
        "    return epoch\n",
        "# =======================================\n",
        "# Train the Model with Enhanced Pipeline\n",
        "# =======================================\n",
        "last_epoch=0\n",
        "last_epoch = train_model_with_accuracy(model, train_loader, val_loader, criterion, optimizer, tokenizer, num_epochs=2)\n",
        "\n",
        "# =======================================\n",
        "#  Evaluation Metrics\n",
        "# =======================================\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "def compute_metrics(predictions, labels, tokenizer):\n",
        "    decoded_preds = [tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions]\n",
        "    decoded_labels = [tokenizer.decode(label, skip_special_tokens=True) for label in labels]\n",
        "\n",
        "    # Flatten and compute metrics\n",
        "    precision = precision_score(decoded_labels, decoded_preds, average='macro', zero_division=0)\n",
        "    recall = recall_score(decoded_labels, decoded_preds, average='macro', zero_division=0)\n",
        "    f1 = f1_score(decoded_labels, decoded_preds, average='macro', zero_division=0)\n",
        "\n",
        "    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
        "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
        "\n",
        "# =======================================\n",
        "#  Save the Model\n",
        "# =======================================\n",
        "model.save_pretrained(f\"./pixelLM_captioning_model_epoch_{last_epoch+1}\")\n",
        "tokenizer.save_pretrained(f\"./pixelLM_captioning_model_epoch_{last_epoch+1}\")\n",
        "model.save_pretrained(\"./image_captioning_model\")\n",
        "tokenizer.save_pretrained(\"./image_captioning_model\")\n",
        "print(\"Model saved successfully!\")\n"
      ]
    }
  ]
}